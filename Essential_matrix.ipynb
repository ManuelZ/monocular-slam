{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# External imports\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from machinevisiontoolbox.Camera import CentralCamera\n",
    "from spatialmath import SE3, SO3\n",
    "\n",
    "# Local imports\n",
    "from slam import Processor\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show pre and post-multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin\n",
    "R0 = np.eye(3)\n",
    "t0 = np.array([0, 0, 0])\n",
    "T0 = SE3.Rt(R0, t0)\n",
    "origin = CentralCamera(pose=T0)\n",
    "origin.plot(scale=0.25, color=\"black\", frame=True)\n",
    "\n",
    "\n",
    "# A new reference frame, start at x=-1\n",
    "Rb = np.eye(3)\n",
    "tb = np.array([-1, 0, 0])\n",
    "Tb = SE3.Rt(Rb, tb)\n",
    "camerab = CentralCamera(pose=Tb)\n",
    "camerab.plot(scale=0.25, color=\"blue\", frame=True)\n",
    "\n",
    "\n",
    "# Pre-multiplication\n",
    "R = SO3.Rz(90, unit=\"deg\")\n",
    "t = np.array([1, 0, 0])\n",
    "T = SE3.Rt(R, t)\n",
    "camerab.pose = T @ camerab.pose\n",
    "camerab.plot(scale=0.25, color=\"green\", frame=True)\n",
    "\n",
    "\n",
    "# Post-multiplication\n",
    "camerab.pose = camerab.pose @ T\n",
    "ax = camerab.plot(scale=0.25, color=\"green\", frame=True)\n",
    "\n",
    "\n",
    "ax.view_init(elev=-90., azim=0, roll=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array(\n",
    "    [[2934.000267, 0.0, 1989.171435],\n",
    "    [0.0, 2935.840316, 948.409835],\n",
    "    [0.0, 0.0, 1.0]]\n",
    ")\n",
    "\n",
    "distortion_coeffs = np.array([[ 0.120889, -0.131111,  0.003076,  0.001933, -0.324229]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Photos:\n",
    "    def __init__(self, base_path, extension=\"*.jpg\"):\n",
    "        self.files = list(Path(base_path).rglob(extension))\n",
    "        self.is_opened = True\n",
    "        self.image_generator = self._image_generator()\n",
    "\n",
    "    def isOpened(self):\n",
    "        return self.is_opened\n",
    "\n",
    "    def release(self):\n",
    "        pass\n",
    "\n",
    "    def _image_generator(self):\n",
    "        \"\"\"Private generator method to yield images.\"\"\"\n",
    "        for impath in self.files:\n",
    "            image = cv2.imread(str(impath))\n",
    "            yield image\n",
    "\n",
    "    def read(self):\n",
    "        try:\n",
    "            return True, next(self.image_generator)\n",
    "        except StopIteration:\n",
    "            return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Photos(\"./images\")\n",
    "\n",
    "frame_ok, image = ds.read()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "frame_ok, image = ds.read()\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options={\"nfeatures\":1000}\n",
    "processor = Processor(K, distortion_coeffs, \"sift\", **options)\n",
    "cap = Photos(\"./images\")\n",
    "\n",
    "\n",
    "# A pose is the extrinsic matrix, a 3x4 matrix\n",
    "# Expressed in the world frame\n",
    "initial_pose = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "\n",
    "pose = initial_pose.copy()\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    frame_ok, frame = cap.read()\n",
    "\n",
    "    if not frame_ok:\n",
    "        break\n",
    "\n",
    "    preprocessed_frame = processor.preprocess_frame(frame, resize=True)\n",
    "    kps, des = processor.detect_and_compute(preprocessed_frame)\n",
    "\n",
    "    if processor.prev[\"frame\"] is None:\n",
    "        processor.prev = {\"frame\": preprocessed_frame, \"kps\": kps, \"des\": des}\n",
    "        continue\n",
    "\n",
    "    prev_matched_pts, curr_matched_pts = processor.get_matched_points(kps, des)\n",
    "\n",
    "    composite = processor.show_matches(\n",
    "        preprocessed_frame, prev_matched_pts, curr_matched_pts, cvshow=False\n",
    "    )\n",
    "    plt.imshow(composite)\n",
    "\n",
    "    R, t = processor.pose_estimation_2d(prev_matched_pts, curr_matched_pts)\n",
    "    T = SE3.Rt(R, t)\n",
    "\n",
    "    # In the code that I've seen, the transform is inverted and post-multiplied, and I don't understand why, either of \n",
    "    # them\n",
    "    # A post-multiplication will produce a translation and then a rotation\n",
    "    T_inv = T.inv().A\n",
    "    pose = pose @ T_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to explain that the Essential Matrix can't be based on planar points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = processor.K\n",
    "\n",
    "# Notice that imagesize takes (w,h) instead of what I'm used to provide (h,w)\n",
    "camera = CentralCamera(\n",
    "    name=\"camera 1\", f=(K[0,0], K[1,1]), imagesize=(processor.w, processor.h), rho=1, pose=T_inv\n",
    ")\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T0 is used as a reference frame, the origin\n",
    "R0 = np.eye(3)\n",
    "t0 = np.array([0, 0, 0])\n",
    "T0 = SE3.Rt(R0, t0)\n",
    "\n",
    "# Plot the initial pose (the origin)\n",
    "origin = CentralCamera(pose=T0)\n",
    "origin.plot(scale=0.25, color=\"black\", frame=True)\n",
    "\n",
    "\n",
    "# Start in a difference place than the origin\n",
    "R1 = SO3.Rz(0, unit=\"deg\")\n",
    "t1 = np.array([0, 1, 0])\n",
    "T1 = SE3.Rt(R1, t1)\n",
    "camera = CentralCamera(pose=T1)\n",
    "camera.plot(scale=0.25, color=\"red\", frame=True)\n",
    "\n",
    "\n",
    "# Plot the transformed pose (pre-multiplied)\n",
    "pose = T.inv() @ T1\n",
    "camera = CentralCamera(pose=pose)\n",
    "camera.plot(scale=0.25, color=\"blue\", frame=True)\n",
    "\n",
    "\n",
    "# GREEN IS THE CORRECT ONE IN MY EXPERIMENT!\n",
    "# Plot the transformed pose (post-multiplied)\n",
    "pose = T1 @ T.inv()\n",
    "camera = CentralCamera(pose=pose)\n",
    "ax = camera.plot(scale=0.25, color=\"green\", frame=True)\n",
    "\n",
    "\n",
    "ax.view_init(elev=-90., azim=0, roll=-90)\n",
    "\n",
    "# Now, with the correct answer at hand, the question is why post-multiplying it and why taking the inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pose or the inverse of the pose?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T0 is the reference frame, the origin\n",
    "R0 = np.eye(3)\n",
    "t0 = np.array([0, 0, 0])\n",
    "T0 = SE3.Rt(R0, t0)\n",
    "\n",
    "\n",
    "camera = CentralCamera(pose=T0)\n",
    "camera.plot(scale=0.25, color=\"black\", frame=True)\n",
    "\n",
    "\n",
    "# Plot the pose\n",
    "camera = CentralCamera(pose=T)\n",
    "ax = camera.plot(scale=0.25, color=\"red\", frame=True)\n",
    "\n",
    "\n",
    "# Plot the inverted pose\n",
    "camera = CentralCamera(pose=T.inv())\n",
    "ax = camera.plot(scale=0.25, color=\"green\", frame=True)\n",
    "\n",
    "\n",
    "ax.view_init(elev=-90., azim=0, roll=-90)\n",
    "\n",
    "# From here, I conclude that recoverPose gives me the pose of Frame A expressed in Frame B.\n",
    "# That's because the inverted pose gives me the correct result: the frame B expressed in Frame A: T_A_B, \n",
    "# hence, inverting the pose gives the pose of Frame A expressed in Frame B: T_B_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, I conclude that `recoverPose` gives me the pose of Frame A expressed in Frame B: `T_B_A`\n",
    "\n",
    "That's because:\n",
    "  - The inverted pose procues the correct result in my experiment: the Frame B expressed in Frame A: `T_A_B` (green frame)\n",
    "  - hence, the pose before invertion was Frame A expressed in Frame B: `T_B_A` (red frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to reconciliate the documentation with my understanding. The documentation says:\n",
    "        \n",
    "    \"(...) this matrix makes up a tuple that performs a change of basis from the first camera's coordinate system to the second camera's coordinate system.\"\n",
    "\n",
    "So, it produces an operator that allows one to re-express a vector in a different frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_B_A = T\n",
    "T_A_B = T_B_A.inv()\n",
    "T_A_B.A @ np.array([[0, 0, 0, 1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
